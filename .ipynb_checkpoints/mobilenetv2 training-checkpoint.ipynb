{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c31b13b-a4b5-4590-a33e-cef75b709292",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1차 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f9af9f-a918-4bbc-916a-f1b0e5c3bd66",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449eb367-b48d-4c2f-a8d0-d08e0ba3cacb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 1. 데이터 경로 설정\n",
    "folder_path = 'sample_2'\n",
    "\n",
    "# 2. 데이터 준비를 위한 리스트\n",
    "images = []\n",
    "landmarks_list = []\n",
    "\n",
    "# 3. 랜드마크 수 정의\n",
    "EXPECTED_LANDMARKS = 106\n",
    "\n",
    "# 4. 폴더 내 모든 파일 처리\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.png'):\n",
    "        # 이미지 로드\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (224, 224))  # 모델 입력 크기로 조정\n",
    "        image = image / 255.0  # 정규화\n",
    "\n",
    "        # 해당 JSON 파일 경로 설정\n",
    "        if filename.endswith('.json'):\n",
    "            json_path = os.path.join(folder_path, json_filename)\n",
    "\n",
    "        if os.path.exists(json_path):\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # 랜드마크 좌표 추출\n",
    "            points = np.array(data['annotation']['points'])\n",
    "\n",
    "            # 랜드마크 수에 따른 처리\n",
    "            num_landmarks = points.shape[0]\n",
    "            if num_landmarks >= EXPECTED_LANDMARKS:\n",
    "                selected_landmarks = points[:EXPECTED_LANDMARKS]  # 첫 106개 랜드마크 사용\n",
    "            else:\n",
    "                selected_landmarks = np.zeros((EXPECTED_LANDMARKS, 2))\n",
    "                selected_landmarks[:num_landmarks] = points  # 기존 랜드마크 넣기\n",
    "                selected_landmarks[num_landmarks:] = np.mean(points, axis=0)  # 평균값으로 채우기\n",
    "\n",
    "            # 정규화\n",
    "            points_normalized = selected_landmarks / [image.shape[1], image.shape[0]]\n",
    "            landmarks = points_normalized.flatten()  # 1D 배열로 변환\n",
    "            \n",
    "            # 이미지와 랜드마크 모두 추가\n",
    "            images.append(image)\n",
    "            landmarks_list.append(landmarks)\n",
    "            \n",
    "# 5. 데이터 배열로 변환\n",
    "images = np.array(images)\n",
    "landmarks_list = np.array(landmarks_list)\n",
    "\n",
    "# 6. 모델 구축 및 학습 과정\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "x = tf.keras.layers.Flatten()(base_model.output)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "output = tf.keras.layers.Dense(EXPECTED_LANDMARKS * 2, activation='linear')(x)  # 136 랜드마크\n",
    "\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# 7. 모델 학습\n",
    "model.fit(images, landmarks_list, epochs=3, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869745b2-6c32-4a49-a45d-2a9dcfbbb102",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. 데이터 경로 설정\n",
    "folder_path = 'sample_2'\n",
    "\n",
    "# 2. 데이터 준비를 위한 리스트\n",
    "images = []\n",
    "landmarks_list = []\n",
    "\n",
    "# 3. 랜드마크 수 정의\n",
    "EXPECTED_LANDMARKS = 106\n",
    "selected_landmarks = np.zeros((EXPECTED_LANDMARKS, 2))\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.json'):\n",
    "        json_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    if filename.endswith('.png'):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    if os.path.exists(json_path):\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        points = np.array(data['annotation']['points'])\n",
    "        num_landmarks = points.shape[0]\n",
    "        \n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.resize(image, (224, 224))  # 모델 입력 크기로 조정\n",
    "        image = image / 255.0  # 정규화\n",
    "\n",
    "\n",
    "        selected_landmarks[:num_landmarks] = points\n",
    "        points_normalized = selected_landmarks / [image.shape[1], image.shape[0]]\n",
    "        landmarks = points_normalized.flatten() \n",
    "        \n",
    "        images.append(image)\n",
    "        landmarks_list.append(landmarks)\n",
    "\n",
    "print(images.shape)\n",
    "\n",
    "# 6. 모델 구축 및 학습 과정\n",
    "# base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "# x = tf.keras.layers.Flatten()(base_model.output)\n",
    "# x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "# output = tf.keras.layers.Dense(EXPECTED_LANDMARKS * 2, activation='linear')(x)  # 136 랜드마크\n",
    "\n",
    "# model = tf.keras.Model(inputs=base_model.input, outputs=output)\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# # 7. 모델 학습\n",
    "# model.fit(images, landmarks_list, epochs=3, batch_size=32)\n",
    "# base_model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0a8a83-d019-4c8b-9d70-06fe5fbebfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save('mobilenetv2_face_detection.h5')  # 원하는 파일 경로와 이름을 입력하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd15950-3477-40dd-86e8-a3cfed053db6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 웹캠 시작\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 랜드마크 수 정의\n",
    "EXPECTED_LANDMARKS = 68\n",
    "\n",
    "# 모델 로드 (모델이 저장된 경로에 맞게 수정하세요)\n",
    "model = tf.keras.models.load_model('mobilenetv2_face_detection.h5')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # 이미지 전처리\n",
    "    img = cv2.resize(frame, (224, 224))  # 모델 입력 크기로 조정\n",
    "    img = img / 255.0  # 정규화\n",
    "    img = np.expand_dims(img, axis=0)  # 배치 차원 추가\n",
    "\n",
    "    # 랜드마크 예측\n",
    "    predicted_landmarks = model.predict(img)\n",
    "    landmarks = predicted_landmarks.reshape((EXPECTED_LANDMARKS, 2)) * np.array([frame.shape[1], frame.shape[0]])  # 원래 크기로 변환\n",
    "\n",
    "    # 예측된 랜드마크를 이미지에 그리기\n",
    "    for (x, y) in landmarks.astype(int):\n",
    "        cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)  # 랜드마크 포인트\n",
    "\n",
    "    # 이미지 표시\n",
    "    cv2.imshow('Landmark Prediction', frame)\n",
    "\n",
    "    # 'q' 키를 눌러 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 자원 해제\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced593f-66de-4680-9376-2e452f3ffed1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 랜드마크 시각화 함수 정의\n",
    "def visualize_landmarks(images, landmarks_list, num_images=3):\n",
    "    for i in range(min(num_images, len(images))):  # 최대 num_images 개수만큼 반복\n",
    "        image = images[i]\n",
    "        landmarks = landmarks_list[i].reshape((EXPECTED_LANDMARKS, 2))  # 2D 배열로 변환\n",
    "        \n",
    "        # 이미지 시각화\n",
    "        plt.imshow(image)\n",
    "        plt.scatter(landmarks[:, 0] * image.shape[1], landmarks[:, 1] * image.shape[0], c='red', s=10)  # 랜드마크 포인트\n",
    "        plt.title(f'Landmarks for Image {i + 1}')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# 랜드마크 시각화 함수 호출\n",
    "visualize_landmarks(images, landmarks_list, num_images=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8f0aa4-7ed5-4f96-b87c-970572abd152",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "base_pitch = None\n",
    "drowsiness_count = 0\n",
    "drowsiness_threshold = 3\n",
    "last_drowsiness_time = time.time()\n",
    "last_count_time = time.time()\n",
    "blink_start_time = None\n",
    "\n",
    "while True:\n",
    "    ret, image = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    image = cv2.resize(image, (800, int(image.shape[0] * 800 / image.shape[1])))\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(image_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            landmarks = []\n",
    "            for landmark in face_landmarks.landmark:\n",
    "                h, w, _ = image.shape\n",
    "                x, y, z = int(landmark.x * w), int(landmark.y * h), landmark.z\n",
    "                landmarks.append((x, y, z))\n",
    "\n",
    "            nose_tip = landmarks[1]\n",
    "            chin = landmarks[152]\n",
    "            pitch = np.arctan2(chin[1] - nose_tip[1], chin[2] - nose_tip[2])\n",
    "            pitch_degrees = np.degrees(pitch)\n",
    "\n",
    "            if base_pitch is None:\n",
    "                base_pitch = pitch_degrees\n",
    "\n",
    "            adjusted_pitch = pitch_degrees - base_pitch\n",
    "            cv2.putText(image, f\"Pitch: {adjusted_pitch:.2f}°\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.putText(image, f\"first: {base_pitch:.2f}°\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "            current_time = time.time()\n",
    "            if adjusted_pitch <= -0:\n",
    "                if current_time - last_drowsiness_time >= 2:\n",
    "                    drowsiness_count += 1\n",
    "                    last_drowsiness_time = current_time\n",
    "                    last_count_time = current_time\n",
    "\n",
    "                if drowsiness_count >= drowsiness_threshold:\n",
    "                    if blink_start_time is None:\n",
    "                        blink_start_time = current_time\n",
    "           \n",
    "            if current_time - last_count_time >= 10:\n",
    "                drowsiness_count = 0\n",
    "\n",
    "            if drowsiness_count >= drowsiness_threshold:\n",
    "                if (time.time() - blink_start_time) % 0.5 < 0.25:\n",
    "                    font_scale = 5\n",
    "                    text = \"sleep\"\n",
    "                    text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 2)[0]\n",
    "                    text_x = (image.shape[1] - text_size[0]) // 2\n",
    "                    text_y = (image.shape[0] + text_size[1]) // 2\n",
    "                    cv2.putText(image, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 255), 2)\n",
    "\n",
    "            cv2.putText(image, f\"Drowsiness Count: {drowsiness_count}\", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow('Face Mesh', image)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3db7da-eb55-410c-ab55-0c6addfeb37c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import dlib\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Mediapipe 초기화\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, min_detection_confidence=0.5)\n",
    "\n",
    "# dlib의 얼굴 탐지기 및 랜드마크 예측기 초기화\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# 웹캠 초기화\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# 고개 기울기 기준값 및 졸음 감지 변수\n",
    "base_pitch = None\n",
    "drowsiness_count = 0\n",
    "drowsiness_threshold = 3\n",
    "last_drowsiness_time = time.time()\n",
    "last_count_time = time.time()\n",
    "blink_start_time = None\n",
    "\n",
    "def get_pitch_and_roll(landmarks):\n",
    "    # 코 끝 (랜드마크 30)과 턱 (랜드마크 8)을 이용해 피치 계산\n",
    "    nose_tip = np.array([landmarks.part(30).x, landmarks.part(30).y])\n",
    "    chin = np.array([landmarks.part(8).x, landmarks.part(8).y])\n",
    "    \n",
    "    # 얼굴 기울기 (피치)\n",
    "    pitch = np.arctan2(chin[1] - nose_tip[1], chin[0] - nose_tip[0])\n",
    "    pitch_degrees = np.degrees(pitch)\n",
    "    \n",
    "    # 두 눈 사이를 기준으로 얼굴의 롤(좌우 기울기) 계산\n",
    "    left_eye = np.array([landmarks.part(36).x, landmarks.part(36).y])\n",
    "    right_eye = np.array([landmarks.part(45).x, landmarks.part(45).y])\n",
    "    eye_delta_y = left_eye[1] - right_eye[1]\n",
    "    eye_delta_x = left_eye[0] - right_eye[0]\n",
    "    roll = np.arctan2(eye_delta_y, eye_delta_x)\n",
    "    roll_degrees = np.degrees(roll)\n",
    "    \n",
    "    return pitch_degrees, roll_degrees\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 이미지 크기 조정 및 Mediapipe 얼굴 랜드마크 추출\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(image_rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            # Mediapipe로 얼굴 랜드마크를 추출하여 화면에 표시\n",
    "            for landmark in face_landmarks.landmark:\n",
    "                h, w, _ = frame.shape\n",
    "                x, y = int(landmark.x * w), int(landmark.y * h)\n",
    "                cv2.circle(frame, (x, y), 1, (0, 255, 0), -1)\n",
    "\n",
    "    # dlib을 이용한 추가 얼굴 랜드마크 처리 (더 정밀한 각도 계산)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)  # dlib 랜드마크 추출\n",
    "\n",
    "        # dlib에서 피치와 롤 계산\n",
    "        pitch_degrees, roll_degrees = get_pitch_and_roll(landmarks)\n",
    "\n",
    "        # 기준 피치 설정 (최초 프레임에서 설정)\n",
    "        if base_pitch is None:\n",
    "            base_pitch = pitch_degrees\n",
    "\n",
    "        # 조정된 피치 값 (기준값과의 차이)\n",
    "        adjusted_pitch = pitch_degrees - base_pitch\n",
    "\n",
    "    # 화면에 피치와 롤 출력\n",
    "    cv2.putText(frame, f\"Pitch: {adjusted_pitch:.2f}°\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(frame, f\"Roll: {roll_degrees:.2f}°\", (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # 졸음 감지 로직\n",
    "    current_time = time.time()\n",
    "    if adjusted_pitch <= -15:  # 고개가 많이 숙여졌을 때\n",
    "        if current_time - last_drowsiness_time >= 2:  # 2초 이상 유지 시 졸음 카운트 증가\n",
    "            drowsiness_count += 1\n",
    "            last_drowsiness_time = current_time\n",
    "            last_count_time = current_time\n",
    "\n",
    "        if drowsiness_count >= drowsiness_threshold:  # 졸음 감지 임계값 초과 시\n",
    "            if blink_start_time is None:\n",
    "                blink_start_time = current_time\n",
    "\n",
    "    # 10초 이상 지나면 졸음 카운트 초기화\n",
    "    if current_time - last_count_time >= 10:\n",
    "        drowsiness_count = 0\n",
    "\n",
    "    # 졸음 경고 표시\n",
    "    if drowsiness_count >= drowsiness_threshold:\n",
    "        if (time.time() - blink_start_time) % 0.5 < 0.25:  # 깜박이는 경고 메시지\n",
    "            font_scale = 5\n",
    "            text = \"SLEEP\"\n",
    "            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 2)[0]\n",
    "            text_x = (frame.shape[1] - text_size[0]) // 2\n",
    "            text_y = (frame.shape[0] + text_size[1]) // 2\n",
    "            cv2.putText(frame, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 255), 2)\n",
    "\n",
    "    # 졸음 카운트 출력\n",
    "    cv2.putText(frame, f\"Drowsiness Count: {drowsiness_count}\", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "    # 결과 이미지 출력\n",
    "    cv2.imshow('Mediapipe + Dlib Face Landmarks', frame)\n",
    "\n",
    "    # 'q'를 누르면 종료\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d4df0-08f4-4249-9f9f-a9590757f8da",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2차 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad614bca-b9c2-4a63-a47a-f0ee7ed003a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# MobileNet 모델 로드 (ImageNet으로 사전 학습된 가중치 사용)\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# GlobalAveragePooling2D로 피처 추출\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# 출력 레이어: 회귀 문제이므로 Dense 레이어에 유닛 하나 추가\n",
    "# (여기서는 머리 각도를 예측하므로 활성화 함수는 사용하지 않음)\n",
    "predictions = Dense(1)(x)\n",
    "\n",
    "# 새로운 모델 정의\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# 사전 학습된 레이어는 고정하고 학습되지 않도록 설정\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18256f6f-76ca-4fc3-a4bf-5028c30a7afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'C:/_AppleBanana/TrainData/GradeData2/train',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='raw',  # 각도 예측이므로 회귀 문제로 설정\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    'C:/_AppleBanana/TrainData/GradeData2/test',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='raw',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945acc4-ffcb-48f6-8c81-7818fac524f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3차 시도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981cb1ca-896e-4e4e-9c0e-e9b9d3cc8f04",
   "metadata": {},
   "source": [
    "## 원본 데이터와 랜드마크 데이터 매칭 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da9c848-3fb5-451f-b136-3712ba161560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "# 이미지 크기 설정 (MobileNet의 기본 입력 크기)\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "def load_data(image_dir, label_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # 이미지와 JSON 파일 매칭을 위해 이미지 파일명 목록 생성\n",
    "    image_files = {}\n",
    "    json_files = {}\n",
    "\n",
    "    # 하위 폴더 포함 이미지 파일 목록 생성\n",
    "    for root, _, files in os.walk(image_dir):\n",
    "        for f in files:\n",
    "            if f.endswith('.png'):\n",
    "                base_filename = f.split('.')[0]\n",
    "                image_files[base_filename] = os.path.join(root, f)\n",
    "\n",
    "    # 하위 폴더 포함 JSON 파일 목록 생성\n",
    "    for root, _, files in os.walk(label_dir):\n",
    "        for f in files:\n",
    "            if f.endswith('.json'):\n",
    "                base_filename = f.split('.')[0]\n",
    "                json_files[base_filename] = os.path.join(root, f)\n",
    "\n",
    "    # 이미지 파일명 기준으로 매칭\n",
    "    for base_filename, img_path in image_files.items():\n",
    "        if base_filename in json_files:\n",
    "            # 이미지 로드 및 전처리\n",
    "            img = load_img(img_path, target_size=IMG_SIZE)\n",
    "            img_array = img_to_array(img) / 255.0  # 정규화\n",
    "            images.append(img_array)\n",
    "\n",
    "            # 해당 이미지의 라벨 로드 및 전처리\n",
    "            label_path = json_files[base_filename]\n",
    "            with open(label_path, 'r') as f:\n",
    "                label_data = json.load(f)\n",
    "                \n",
    "                # 이미지 원본 크기 정보 (width, height)\n",
    "                orig_width = label_data['width']\n",
    "                orig_height = label_data['height']\n",
    "                \n",
    "                # 첫 번째 annotation 정보에서 랜드마크 추출\n",
    "                landmark = label_data['annotation'][0]['landmark']\n",
    "                \n",
    "                # 원본 크기에 대한 랜드마크 좌표를 리사이징된 이미지 크기에 맞게 변환\n",
    "                resized_landmarks = []\n",
    "                for (x, y) in landmark:\n",
    "                    resized_x = (x / orig_width) * IMG_SIZE[0]\n",
    "                    resized_y = (y / orig_height) * IMG_SIZE[1]\n",
    "                    resized_landmarks.extend([resized_x, resized_y])  # (x, y) 좌표를 1D 리스트로 확장\n",
    "                \n",
    "                labels.append(resized_landmarks)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f7ca7-1ffc-42dd-8aab-9f5636c800ad",
   "metadata": {},
   "source": [
    "## 데이터셋 폴더 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fafaa70-a331-4c7f-8357-2e201aec3537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "image_dir = 'C:/_AppleBanana/TrainData/GradeData2/train'\n",
    "label_dir = 'C:/_AppleBanana/TrainData/GradeData2/test'\n",
    "\n",
    "# 데이터 로드\n",
    "X, y = load_data(image_dir, label_dir)\n",
    "\n",
    "# 데이터 형태 확인\n",
    "print(f\"이미지 데이터 형태: {X.shape}\")\n",
    "print(f\"라벨 데이터 형태: {y.shape}\")\n",
    "\n",
    "# 줄어든 이미지 하나 시각화 (첫 번째 이미지 사용)\n",
    "def show_image_with_landmarks(img, landmarks):\n",
    "    plt.imshow(img)\n",
    "    landmarks = np.array(landmarks).reshape(-1, 2)  # (x, y) 쌍으로 변환\n",
    "    plt.scatter(landmarks[:, 0], landmarks[:, 1], c='r', marker='x')  # 빨간색 x로 랜드마크 표시\n",
    "    plt.show()\n",
    "\n",
    "# 첫 번째 이미지와 랜드마크 표시\n",
    "show_image_with_landmarks(X[1000], y[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb03dd-3b5c-4485-abf4-674dd2323ac9",
   "metadata": {},
   "source": [
    "## 초기 모델 학습 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf7bb3-a9db-4037-aa98-6ac453761c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# MobileNetV2 모델 불러오기 (ImageNet weights 사용, 마지막 layer 제외)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # 사전 훈련된 가중치를 고정 X\n",
    "\n",
    "# 모델 정의\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(1024, activation='relu'),  # L2 정규화\n",
    "    layers.BatchNormalization(),  # 배치 정규화\n",
    "    layers.Dropout(0.3),  # 50% 드롭아웃 적용\n",
    "    layers.Dense(10)  # 5개의 랜드마크, 각 (x, y) 좌표\n",
    "])\n",
    "\n",
    "def smooth_l1_loss(y_true, y_pred):\n",
    "    diff = tf.abs(y_true - y_pred)\n",
    "    less_than_one = tf.cast(tf.less(diff, 1.0), tf.float32)\n",
    "    loss = less_than_one * 0.5 * diff**2 + (1.0 - less_than_one) * (diff - 0.5)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# 조기 종료 설정 (검증 손실이 5 epoch 동안 향상되지 않으면 학습 중단)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# 학습률이 향상되지 않으면 학습률 감소\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X, y, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# 모델 저장\n",
    "model.save('mobilenetv2_face_detection.h5')  # 원하는 파일 경로와 이름을 입력하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739a7e96-2c89-486a-850a-9a01776ae39c",
   "metadata": {},
   "source": [
    "## 추가 데이터셋 폴더 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2106ffb2-858e-4d50-9e1c-2b57be3f91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "image_dir = 'D:/TrainData/GradeData1_full/Validation/train'\n",
    "label_dir = 'D:/TrainData/GradeData1_full/Validation/test'\n",
    "\n",
    "# 데이터 로드\n",
    "X, y = load_data(image_dir, label_dir)\n",
    "\n",
    "# 데이터 형태 확인\n",
    "print(f\"이미지 데이터 형태: {X.shape}\")\n",
    "print(f\"라벨 데이터 형태: {y.shape}\")\n",
    "\n",
    "# 줄어든 이미지 하나 시각화 (첫 번째 이미지 사용)\n",
    "def show_image_with_landmarks(img, landmarks):\n",
    "    plt.imshow(img)\n",
    "    landmarks = np.array(landmarks).reshape(-1, 2)  # (x, y) 쌍으로 변환\n",
    "    plt.scatter(landmarks[:, 0], landmarks[:, 1], c='r', marker='x')  # 빨간색 x로 랜드마크 표시\n",
    "    plt.show()\n",
    "\n",
    "# 첫 번째 이미지와 랜드마크 표시\n",
    "# show_image_with_landmarks(X[0], y[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf42a3-2e2b-4928-b814-884807628fe7",
   "metadata": {},
   "source": [
    "## 모델 추가 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb09128b-acba-4d35-89a9-09610309cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# 저장된 모델 불러오기 (compile=False로 설정)\n",
    "model = tf.keras.models.load_model('mobilenetv2_face_detection_tuning_GD1.h5', compile=False)\n",
    "\n",
    "# Fine-tuning을 위해 특정 레이어의 학습을 허용 (예: 마지막 몇 개 레이어)\n",
    "base_model = model.layers[0]  # MobileNetV2 base model\n",
    "base_model.trainable = True  # MobileNetV2 모델의 모든 레이어 학습 가능하도록 설정\n",
    "\n",
    "# 모델 재컴파일\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# 조기 종료 및 학습률 감소 콜백 재설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "\n",
    "# 추가 학습\n",
    "model.fit(X, y, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# 모델 다시 저장\n",
    "model.save('mobilenetv2_face_detection_tuning_GD1.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31483876-a1cd-4b7d-83b2-59ee7a58d0eb",
   "metadata": {},
   "source": [
    "## 랜드마크 예측 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919bde25-bf3c-4829-98fc-25801649c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 후 예측된 랜드마크 시각화\n",
    "def predict_and_show(model, img):\n",
    "    img = np.expand_dims(img, axis=0)  # 배치 차원 추가\n",
    "    pred_landmarks = model.predict(img)\n",
    "    show_image_with_landmarks(img[0], pred_landmarks[0])\n",
    "    show_image_with_landmarks(X[1], y[1])\n",
    "\n",
    "# 첫 번째 이미지에 대한 예측 결과 시각화\n",
    "predict_and_show(model, X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeea590-563e-432d-a074-2198f76c6bed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4차 시도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db44ceec-bbe6-428a-9b41-1dac023e6f2f",
   "metadata": {},
   "source": [
    "## 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a5ff4-26e6-4599-8ac8-bfcf6e638cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b55437f-8c82-4326-b447-4774c4543bd9",
   "metadata": {},
   "source": [
    "## 함수 모음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da3ad9-8368-4d00-ae39-5adcb280963a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 이미지 크기 설정 (MobileNet의 기본 입력 크기)\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "def load_data(image_dir, label_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # 이미지와 JSON 파일 매칭을 위해 이미지 파일명 목록 생성\n",
    "    image_files = {}\n",
    "    json_files = {}\n",
    "\n",
    "    # 하위 폴더 포함 이미지 파일 목록 생성\n",
    "    for root, _, files in os.walk(image_dir):\n",
    "        for f in files:\n",
    "            if f.endswith('.png'):\n",
    "                base_filename = f.split('.')[0]\n",
    "                image_files[base_filename] = os.path.join(root, f)\n",
    "\n",
    "    # 하위 폴더 포함 JSON 파일 목록 생성\n",
    "    for root, _, files in os.walk(label_dir):\n",
    "        for f in files:\n",
    "            if f.endswith('.json'):\n",
    "                base_filename = f.split('.')[0]\n",
    "                json_files[base_filename] = os.path.join(root, f)\n",
    "\n",
    "    # 이미지 파일명 기준으로 매칭\n",
    "    for base_filename, img_path in image_files.items():\n",
    "        if base_filename in json_files:\n",
    "            # 이미지 로드 및 전처리\n",
    "            img = load_img(img_path, target_size=IMG_SIZE)\n",
    "            img_array = img_to_array(img) / 255.0  # 정규화\n",
    "            images.append(img_array)\n",
    "\n",
    "            # 해당 이미지의 라벨 로드 및 전처리\n",
    "            label_path = json_files[base_filename]\n",
    "            with open(label_path, 'r') as f:\n",
    "                label_data = json.load(f)\n",
    "                \n",
    "                # 이미지 원본 크기 정보 (width, height)\n",
    "                orig_width = label_data['width']\n",
    "                orig_height = label_data['height']\n",
    "                \n",
    "                # 첫 번째 annotation 정보에서 랜드마크 추출\n",
    "                landmark = label_data['annotation'][0]['landmark']\n",
    "                \n",
    "                # 원본 크기에 대한 랜드마크 좌표를 리사이징된 이미지 크기에 맞게 변환\n",
    "                resized_landmarks = []\n",
    "                for (x, y) in landmark:\n",
    "                    resized_x = (x / orig_width) * IMG_SIZE[0]\n",
    "                    resized_y = (y / orig_height) * IMG_SIZE[1]\n",
    "                    resized_landmarks.extend([resized_x, resized_y])  # (x, y) 좌표를 1D 리스트로 확장\n",
    "                \n",
    "                labels.append(resized_landmarks)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# 줄어든 이미지 하나 시각화 (첫 번째 이미지 사용)\n",
    "def show_image_with_landmarks(img, landmarks):\n",
    "    plt.imshow(img)\n",
    "    landmarks = np.array(landmarks).reshape(-1, 2)  # (x, y) 쌍으로 변환\n",
    "    plt.scatter(landmarks[:, 0], landmarks[:, 1], c='r', marker='x')  # 빨간색 x로 랜드마크 표시\n",
    "    plt.show()\n",
    "\n",
    "def smooth_l1_loss(y_true, y_pred):\n",
    "    diff = tf.abs(y_true - y_pred)\n",
    "    less_than_one = tf.cast(tf.less(diff, 1.0), tf.float32)\n",
    "    loss = less_than_one * 0.5 * diff**2 + (1.0 - less_than_one) * (diff - 0.5)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "# 줄어든 이미지 하나 시각화 (첫 번째 이미지 사용)\n",
    "def show_image_with_landmarks(img, landmarks):\n",
    "    plt.imshow(img)\n",
    "    landmarks = np.array(landmarks).reshape(-1, 2)  # (x, y) 쌍으로 변환\n",
    "    plt.scatter(landmarks[:, 0], landmarks[:, 1], c='r', marker='x')  # 빨간색 x로 랜드마크 표시\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb28368c-cc5e-48e6-8581-b119501f6a19",
   "metadata": {},
   "source": [
    "## 데이터셋 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc42a826-577d-4721-bedf-c40df63e37bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "image_dir = 'D:/TrainData/GradeData1_full/Validation/train'\n",
    "label_dir = 'D:/TrainData/GradeData1_full/Validation/test'\n",
    "\n",
    "# 데이터 로드\n",
    "X, y = load_data(image_dir, label_dir)\n",
    "\n",
    "# 데이터 형태 확인\n",
    "print(f\"이미지 데이터 형태: {X.shape}\")\n",
    "print(f\"라벨 데이터 형태: {y.shape}\")\n",
    "\n",
    "# 첫 번째 이미지와 랜드마크 표시\n",
    "show_image_with_landmarks(X[1000], y[1000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b6237f-072d-4bb3-9e99-11daaa0fc27c",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c9cf08-ec5f-4906-9808-b725ae9451b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV2 모델 불러오기 (ImageNet weights 사용, 마지막 layer 제외)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # 사전 훈련된 가중치를 고정 X\n",
    "\n",
    "# 모델 정의\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(1024, activation='relu'),  # L2 정규화\n",
    "    layers.BatchNormalization(),  # 배치 정규화\n",
    "    layers.Dropout(0.3),  # 50% 드롭아웃 적용\n",
    "    layers.Dense(10)  # 5개의 랜드마크, 각 (x, y) 좌표\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# 조기 종료 설정 (검증 손실이 5 epoch 동안 향상되지 않으면 학습 중단)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# 학습률이 향상되지 않으면 학습률 감소\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.00001)\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X, y, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# 모델 저장\n",
    "model.save('mobilenetv2_GD1_F_V.h5')  # 원하는 파일 경로와 이름을 입력하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b2690-900e-41b9-8e32-8aff6e7b0598",
   "metadata": {},
   "source": [
    "## 모델 추가 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6913d4a-834c-4005-9ba3-54af6faf1c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 불러오기 (compile=False로 설정)\n",
    "model = tf.keras.models.load_model('mobilenetv2_GD1_F_V.h5', compile=False)\n",
    "\n",
    "# Fine-tuning을 위해 특정 레이어의 학습을 허용 (예: 마지막 몇 개 레이어)\n",
    "base_model = model.layers[0]  # MobileNetV2 base model\n",
    "base_model.trainable = True  # MobileNetV2 모델의 모든 레이어 학습 가능하도록 설정\n",
    "\n",
    "# 모델 재컴파일\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# 조기 종료 및 학습률 감소 콜백 재설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "\n",
    "# 추가 학습\n",
    "model.fit(X, y, epochs=30, batch_size=32, validation_split=0.2, callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# 모델 다시 저장\n",
    "model.save('mobilenetv2_GD1_F_V.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f922c-f75c-4ca4-9ea8-78f21b5d1599",
   "metadata": {},
   "source": [
    "## 예측 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8d1623-4b80-4575-9228-f78feb740a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 후 예측된 랜드마크 시각화\n",
    "def predict_and_show(model, img):\n",
    "    img = np.expand_dims(img, axis=0)  # 배치 차원 추가\n",
    "    pred_landmarks = model.predict(img)\n",
    "    show_image_with_landmarks(img[0], pred_landmarks[0])\n",
    "    show_image_with_landmarks(X[10], y[10])\n",
    "\n",
    "# 첫 번째 이미지에 대한 예측 결과 시각화\n",
    "predict_and_show(model, X[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f030ff0-f465-49a3-b10d-b061a4268bec",
   "metadata": {},
   "source": [
    "# 5차 시도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780c034d-e1e4-4a89-948d-95d5ea835abc",
   "metadata": {},
   "source": [
    "## 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6ddf1f-24c8-4fce-bdc4-d40b4552938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214cb222-82be-461d-ab78-484001d5f74e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 데이터 로딩 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c775099-0224-4c18-b56e-be9dc17feadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(image_dir, label_dir):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # 이미지와 JSON 파일 매칭을 위해 이미지 파일명 목록 생성\n",
    "    image_files = {}\n",
    "    json_files = {}\n",
    "\n",
    "    # 하위 폴더 포함 이미지 파일 목록 생성\n",
    "    for root, _, files in os.walk(image_dir):\n",
    "        for f in files:\n",
    "            if f.endswith('.png'):\n",
    "                base_filename = f.split('.')[0]\n",
    "                image_files[base_filename] = os.path.join(root, f)\n",
    "\n",
    "    # 하위 폴더 포함 JSON 파일 목록 생성\n",
    "    for root, _, files in os.walk(label_dir):\n",
    "        for f in files:\n",
    "            if f.endswith('.json'):\n",
    "                base_filename = f.split('.')[0]\n",
    "                json_files[base_filename] = os.path.join(root, f)\n",
    "\n",
    "    # 이미지 파일명 기준으로 매칭\n",
    "    for base_filename, img_path in image_files.items():\n",
    "        if base_filename in json_files:\n",
    "            # 이미지 로드 및 전처리\n",
    "            img = load_img(img_path, target_size=IMG_SIZE)\n",
    "            img_array = img_to_array(img) / 255.0  # 정규화\n",
    "            images.append(img_array)\n",
    "\n",
    "            # 해당 이미지의 라벨 로드 및 전처리\n",
    "            label_path = json_files[base_filename]\n",
    "            with open(label_path, 'r') as f:\n",
    "                label_data = json.load(f)\n",
    "                \n",
    "                # 이미지 원본 크기 정보 (width, height)\n",
    "                orig_width = label_data['width']\n",
    "                orig_height = label_data['height']\n",
    "                \n",
    "                # 첫 번째 annotation 정보에서 랜드마크 추출\n",
    "                landmark = label_data['annotation'][0]['landmark']\n",
    "                \n",
    "                # 원본 크기에 대한 랜드마크 좌표를 리사이징된 이미지 크기에 맞게 변환\n",
    "                resized_landmarks = []\n",
    "                for (x, y) in landmark:\n",
    "                    resized_x = (x / orig_width) * IMG_SIZE[0]\n",
    "                    resized_y = (y / orig_height) * IMG_SIZE[1]\n",
    "                    resized_landmarks.extend([resized_x, resized_y])  # (x, y) 좌표를 1D 리스트로 확장\n",
    "                \n",
    "                labels.append(resized_landmarks)\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e25fb-7ca0-48ac-9525-ac2a6a13cb8c",
   "metadata": {},
   "source": [
    "## 데이터 경로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e43e0045-cbac-49c9-ad02-347b47decad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\_AppleBanana\\WatchOutDriverModel_MobileNet\\myenv\\Lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (108000000 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "C:\\_AppleBanana\\WatchOutDriverModel_MobileNet\\myenv\\Lib\\site-packages\\PIL\\Image.py:3368: DecompressionBombWarning: Image size (133505320 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 훈련 및 검증 데이터셋 로드\u001b[39;00m\n\u001b[0;32m     12\u001b[0m train_images, train_labels \u001b[38;5;241m=\u001b[39m load_data(train_img_dir, train_label_dir)\n\u001b[1;32m---> 13\u001b[0m val_images, val_labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_img_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_label_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain images shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_images\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# (N, 224, 224, 3)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain labels shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_labels\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# (N, 10)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(image_dir, label_dir)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m base_filename, img_path \u001b[38;5;129;01min\u001b[39;00m image_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m base_filename \u001b[38;5;129;01min\u001b[39;00m json_files:\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;66;03m# 이미지 로드 및 전처리\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m         img_array \u001b[38;5;241m=\u001b[39m img_to_array(img) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# 정규화\u001b[39;00m\n\u001b[0;32m     29\u001b[0m         images\u001b[38;5;241m.\u001b[39mappend(img_array)\n",
      "File \u001b[1;32mC:\\_AppleBanana\\WatchOutDriverModel_MobileNet\\myenv\\Lib\\site-packages\\keras\\src\\utils\\image_utils.py:236\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, color_mode, target_size, interpolation, keep_aspect_ratio)\u001b[0m\n\u001b[0;32m    234\u001b[0m         path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(path\u001b[38;5;241m.\u001b[39mresolve())\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m--> 236\u001b[0m         img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be path-like or io.BytesIO, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    240\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 이미지 크기 및 배치 사이즈 설정\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 데이터 경로 설정\n",
    "train_img_dir = 'D:/TrainData/GradeData1_full/Training/train'\n",
    "train_label_dir = 'D:/TrainData/GradeData1_full/Training/test'\n",
    "val_img_dir = 'D:/TrainData/GradeData1_full/Validation/train'\n",
    "val_label_dir = 'D:/TrainData/GradeData1_full/Validation/test'\n",
    "\n",
    "# 훈련 및 검증 데이터셋 로드\n",
    "train_images, train_labels = load_data(train_img_dir, train_label_dir)\n",
    "val_images, val_labels = load_data(val_img_dir, val_label_dir)\n",
    "\n",
    "print(\"Train images shape:\", train_images.shape)  # (N, 224, 224, 3)\n",
    "print(\"Train labels shape:\", train_labels.shape)  # (N, 10)\n",
    "print(\"Validation images shape:\", val_images.shape)  # (M, 224, 224, 3)\n",
    "print(\"Validation labels shape:\", val_labels.shape)  # (M, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ead9c-231a-4126-9d10-ff92d0ec05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구성\n",
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # 사전 학습된 가중치 고정\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    base_model, \n",
    "    layers.GlobalAveragePooling2D(), \n",
    "    layers.Dense(128, activation='relu'), \n",
    "    layers.Dense(10)  # 5개의 랜드마크 좌표 (x, y) 5쌍이므로 출력 뉴런 수는 10\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mean_absolute_error'])\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(train_images, train_labels, validation_data=(val_images, val_labels), batch_size=BATCH_SIZE, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
